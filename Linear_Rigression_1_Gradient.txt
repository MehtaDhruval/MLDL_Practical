# ============================================================
#     LINEAR REGRESSION USING GRADIENT DESCENT (ONE FILE)
# ============================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# --------------------------------------------------------------
# 1. CREATE DATAFRAME (NO CSV FILE NEEDED)
# --------------------------------------------------------------
data = pd.DataFrame({
    'Age': [10,12,13,14,15,16,17,18,19,20,
            21,22,23,24,25,26,27,28,29,30],
    'Weight': [30,32,35,38,40,42,45,47,49,52,
               54,56,57,59,60,62,63,65,66,68]
})

print("\n=== DATASET ===")
print(data)

# --------------------------------------------------------------
# 2. TAKE X AND Y
# --------------------------------------------------------------
X = np.array(data["Age"])
y = np.array(data["Weight"])

# X and y must be column vectors for gradient descent
X = X.reshape(-1, 1)
y = y.reshape(-1, 1)

# Plot original data
plt.scatter(X, y, color='red')
plt.xlabel("Age")
plt.ylabel("Weight")
plt.title("Original Data Points")
plt.show()

# --------------------------------------------------------------
# 3. INITIAL VALUES
# --------------------------------------------------------------
m = 0      # slope
c = 0      # intercept
L = 0.0001 # learning rate
epochs = 1000
n = len(X)

# --------------------------------------------------------------
# 4. GRADIENT DESCENT FORMULA
# --------------------------------------------------------------
for i in range(epochs):

    y_pred = m * X + c

    # Derivatives
    D_m = (-2/n) * sum(X * (y - y_pred))
    D_c = (-2/n) * sum(y - y_pred)

    # Update m and c
    m = m - L * D_m
    c = c - L * D_c

print("\n=== FINAL VALUES AFTER GRADIENT DESCENT ===")
print("Slope (m) =", m)
print("Intercept (c) =", c)

# --------------------------------------------------------------
# 5. FINAL PREDICTED LINE
# --------------------------------------------------------------
y_pred_final = m * X + c

# Plot regression line
plt.scatter(X, y, color='red', label="Actual Data")
plt.plot(X, y_pred_final, color='blue', label="Predicted Line")
plt.xlabel("Age")
plt.ylabel("Weight")
plt.title("Linear Regression using Gradient Descent")
plt.legend()
plt.show()

# --------------------------------------------------------------
# 6. EXAMPLE PREDICTION
# --------------------------------------------------------------
age_25_weight = m * 25 + c
print("\nPredicted Weight at Age 25 =", age_25_weight)




/*Explantion***************************/
1) Problem setup â€” what we want to do

We have pairs (x_i, y_i) where x = Age and y = Weight.
Goal: find a straight line y = mÂ·x + c that best fits the points in a least-squares sense.

m = slope (how much y changes per unit change in x)

c = intercept (value of y when x = 0)

We will use gradient descent to find m and c that minimize mean squared error (MSE).

2) Data creation and shape (why reshape matters)

Code:

data = pd.DataFrame({'Age': [...], 'Weight': [...]})
X = np.array(data["Age"]).reshape(-1,1)
y = np.array(data["Weight"]).reshape(-1,1)


Why reshape(-1,1)?

We want X and y as column vectors (shape (n,1)) so elementwise operations and broadcasting behave predictably.

Wrong: reshape(1,-1) makes a 1Ã—n row vector, breaking math (and sum / broadcasting).

Check shapes in code while debugging:

print(X.shape, y.shape)  # (20,1) (20,1)

3) Visualize the data

Plot: plt.scatter(X,y).
Why? To see whether a straight line is sensible (it is here). Visualization also helps detect outliers.

4) Loss function (what we're minimizing)

We use Mean Squared Error (MSE):

ğ½
(
ğ‘š
,
ğ‘
)
=
1
ğ‘›
âˆ‘
ğ‘–
=
1
ğ‘›
(
ğ‘¦
ğ‘–
âˆ’
(
ğ‘š
ğ‘¥
ğ‘–
+
ğ‘
)
)
2
J(m,c)=
n
1
	â€‹

i=1
âˆ‘
n
	â€‹

(y
i
	â€‹

âˆ’(mx
i
	â€‹

+c))
2

We want to minimize J(m,c) with respect to m and c.

5) Compute gradients (derivative of loss)

Take derivatives of J:

âˆ‚
ğ½
âˆ‚
ğ‘š
=
âˆ’
2
ğ‘›
âˆ‘
ğ‘–
=
1
ğ‘›
ğ‘¥
ğ‘–
(
ğ‘¦
ğ‘–
âˆ’
(
ğ‘š
ğ‘¥
ğ‘–
+
ğ‘
)
)
âˆ‚m
âˆ‚J
	â€‹

=âˆ’
n
2
	â€‹

i=1
âˆ‘
n
	â€‹

x
i
	â€‹

(y
i
	â€‹

âˆ’(mx
i
	â€‹

+c))
âˆ‚
ğ½
âˆ‚
ğ‘
=
âˆ’
2
ğ‘›
âˆ‘
ğ‘–
=
1
ğ‘›
(
ğ‘¦
ğ‘–
âˆ’
(
ğ‘š
ğ‘¥
ğ‘–
+
ğ‘
)
)
âˆ‚c
âˆ‚J
	â€‹

=âˆ’
n
2
	â€‹

i=1
âˆ‘
n
	â€‹

(y
i
	â€‹

âˆ’(mx
i
	â€‹

+c))

These give the direction to change m and c to reduce the loss.

6) Gradient descent update rule

Using learning rate L (often called alpha):

m := m - L * (âˆ‚J/âˆ‚m)
c := c - L * (âˆ‚J/âˆ‚c)


You repeat this update for many iterations (epochs).

Important: the sign is minus because we move opposite the gradient to decrease loss.

7) Implementation (vectorized)

Vectorized code used in the file:

y_pred = m * X + c              # shape (n,1)
D_m = (-2/n) * sum(X * (y - y_pred))
D_c = (-2/n) * sum(y - y_pred)

m = m - L * D_m
c = c - L * D_c


Notes:

X * (y - y_pred) multiplies each residual by corresponding x_i (broadcasting).

sum(...) here returns a 1Ã—1 array (because shapes are (n,1)); works fine if you treat m and c as arrays/scalars consistently.

This is efficient (no Python loops over data points) and numerically stable for moderate sizes.

8) Choosing hyperparameters: learning rate L and epochs

L too large â†’ updates overshoot the minimum â†’ divergence or oscillation.

L too small â†’ extremely slow convergence.

Typical strategy: try L in [1e-4, 1e-2] for simple problems, adjust by observing loss curve.

epochs: number of iterations. If loss hasn't decreased sufficiently, increase epochs or raise L slightly.

Practical tip: monitor loss during training:

loss_history = []
for i in range(epochs):
    y_pred = m*X + c
    loss = np.mean((y - y_pred)**2)
    loss_history.append(loss)
    # compute gradients, update...


Plot loss_history to check convergence.

9) Final line and prediction

After training we compute:

y_pred_final = m * X + c
plt.plot(X, y_pred_final)


To predict a new x_new, compute m * x_new + c (scalar math).

10) Why you saw y_pred not change (common bugs)

If your predictions stay constant:

You may have wrong shapes (reshape(1,-1)) so sum(...) or broadcasting behaves unexpectedly.

Learning rate may be zero or extremely small.

Not actually updating m and c in the loop (typo).

Using integer arrays causing truncation (use floats).

Check for these by printing interim values:

if i % 100 == 0:
    print(i, float(m), float(c), loss)

11) Evaluate model: MSE, MAE, RÂ²

Add metrics to measure performance:

from sklearn.metrics import mean_squared_error, r2_score

mse = mean_squared_error(y, y_pred_final)
r2 = r2_score(y, y_pred_final)
print("MSE:", mse, "R2:", r2)


RÂ² near 1 means a good linear fit.

12) Compare with closed-form (Normal Equation) or sklearn

Closed-form solution (only for small problems, O(nÂ³) due to matrix inverse):

ğœƒ
=
(
ğ‘‹
ğ‘
ğ‘¢
ğ‘”
ğ‘‡
ğ‘‹
ğ‘
ğ‘¢
ğ‘”
)
âˆ’
1
ğ‘‹
ğ‘
ğ‘¢
ğ‘”
ğ‘‡
ğ‘¦
Î¸=(X
aug
T
	â€‹

X
aug
	â€‹

)
âˆ’1
X
aug
T
	â€‹

y

where X_aug = [1 x] includes bias. Or use sklearn.linear_model.LinearRegression() which uses optimized solvers.

Sklearn snippet:

from sklearn.linear_model import LinearRegression
reg = LinearRegression().fit(X, y)
print(reg.coef_, reg.intercept_)


They should match (approximately) the m, c from gradient descent if GD converged.

13) Improvements & practical tips

Feature scaling / normalization: scale X to zero mean and unit variance when features vary widely â€” helps gradient descent converge faster.

Mini-batch or stochastic gradient descent: for large datasets, update on small batches or single examples for speed.

Learning rate schedule: reduce L over time (e.g., multiply by 0.9 every N epochs) to fine tune.

Early stopping: stop when validation loss stops improving.
