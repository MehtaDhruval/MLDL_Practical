# ============================================================
#   SINGLE-LAYER PERCEPTRON ‚Üí MLP ‚Üí DEEP NEURAL NETWORK
# ============================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# ------------------------------------------------------------
# 1) GENERATE SYNTHETIC BINARY CLASSIFICATION DATA
# ------------------------------------------------------------
X, y = make_classification(
    n_samples=1000,
    n_features=20,
    n_informative=15,
    n_classes=2,
    random_state=42
)

# Train‚Äìtest split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ------------------------------------------------------------
# 2) FEATURE SCALING (Very Important for NN performance)
# ------------------------------------------------------------
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# ============================================================
#   PART 1 ‚Äî SINGLE LAYER PERCEPTRON (SLP)
# ============================================================
print("\n===== SINGLE LAYER PERCEPTRON =====")

slp = Sequential([
    Input(shape=(X_train.shape[1],)),
    Dense(1, activation='sigmoid')
])

slp.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
slp.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)

y_pred_slp = slp.predict(X_test).round()
acc_slp = accuracy_score(y_test, y_pred_slp)

print("Single Layer Perceptron Accuracy:", acc_slp)

# ============================================================
#   PART 2 ‚Äî SINGLE LAYER PERCEPTRON WITH SGD + MSE
# ============================================================
print("\n===== PERCEPTRON (SGD + MSE) =====")

slp2 = Sequential([
    Input(shape=(X_train.shape[1],)),
    Dense(1, activation='sigmoid')
])

slp2.compile(optimizer='SGD', loss='MSE', metrics=['accuracy'])
slp2.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)

y_pred_slp2 = slp2.predict(X_test).round()
acc_slp2 = accuracy_score(y_test, y_pred_slp2)

print("SGD+MSE Perceptron Accuracy:", acc_slp2)

# ============================================================
#   PART 3 ‚Äî MULTI-LAYER PERCEPTRON (2 Hidden Layers)
# ============================================================
print("\n===== MULTI-LAYER PERCEPTRON (MLP) =====")

mlp = Sequential([
    Input(shape=(X_train.shape[1],)),
    Dense(64, activation='relu'),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')
])

mlp.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
mlp.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)

y_pred_mlp = mlp.predict(X_test).round()
acc_mlp = accuracy_score(y_test, y_pred_mlp)

print("MLP Accuracy:", acc_mlp)

# ============================================================
#   PART 4 ‚Äî DEEP NEURAL NETWORK (Many Hidden Layers)
# ============================================================
print("\n===== DEEP NEURAL NETWORK (DNN) =====")

dnn = Sequential([
    Input(shape=(X_train.shape[1],)),
    Dense(128, activation='relu'),
    Dense(128, activation='relu'),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')
])

dnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
dnn.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)

y_pred_dnn = dnn.predict(X_test).round()
acc_dnn = accuracy_score(y_test, y_pred_dnn)

print("Deep Neural Network Accuracy:", acc_dnn)

# ============================================================
# FINAL SUMMARY
# ============================================================
print("\n===== ACCURACY SUMMARY =====")
print(f"Single Layer Perceptron:        {acc_slp:.3f}")
print(f"Perceptron (SGD + MSE):         {acc_slp2:.3f}")
print(f"Multi-Layer Perceptron (MLP):   {acc_mlp:.3f}")
print(f"Deep Neural Network (DNN):      {acc_dnn:.3f}")


/*****************Explnation*********************/
1) Synthetic Dataset

We use:

make_classification()


to create a binary classification dataset with 20 features.

‚úî 2) Standardization is REQUIRED

Neural networks behave poorly when features have different scales.

We do:

StandardScaler()

‚úî 3) Single-Layer Perceptron (SLP)

This is basically logistic regression implemented as a neural network:

Input ‚Üí Dense(1, sigmoid)


It learns:

ùë¶
=
ùúé
(
ùë§
ùëá
ùë•
)
y=œÉ(w
T
x)

Accuracy is low (~0.60) because model is too simple.

‚úî 4) SLP using SGD + MSE

Uses:

Optimizer = SGD

Loss = MSE (not good for classification)

Accuracy drops ‚Üí (~0.57)

This demonstrates why we use binary_crossentropy for classification.

‚úî 5) Multi-Layer Perceptron (MLP)

Now we add hidden layers:

64 ‚Üí 64 ‚Üí sigmoid


This learns non-linear functions.

Accuracy jumps to ~0.90.

‚úî 6) Deep Neural Network (DNN)

We add many layers:

128 ‚Üí 128 ‚Üí 128 ‚Üí 64 ‚Üí 64 ‚Üí sigmoid


This model is more expressive, captures complex patterns.

Accuracy ~0.93+

‚úî KEY OBSERVATION
Model	Accuracy
Perceptron	~0.64
Perceptron (SGD + MSE)	~0.58
MLP	~0.91
DNN	~0.93

‚û° More layers = better learning
‚û° But also more likely to overfit if dataset is small
