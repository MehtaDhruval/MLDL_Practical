# ============================================================
#     EfficientNetB0 Transfer Learning on CIFAR-10 Dataset
# ============================================================

import os, random, numpy as np, tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.metrics import (
    classification_report, confusion_matrix,
    roc_auc_score, precision_recall_fscore_support
)
from sklearn.model_selection import train_test_split

# ------------------------------------------------------------
# 1. Set Seeds
# ------------------------------------------------------------
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

# ------------------------------------------------------------
# 2. Load CIFAR-10 Dataset
# ------------------------------------------------------------
num_classes = 10
base_input_shape = (32, 32, 3)

(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()

y_train = y_train.flatten()
y_test = y_test.flatten()

# Train/Validation split
x_train, x_val, y_train, y_val = train_test_split(
    x_train, y_train,
    test_size=0.1,
    random_state=SEED,
    stratify=y_train
)

# CIFAR-10 is already RGB; just a safety check
if x_train.shape[-1] == 1:
    x_train = np.repeat(x_train, 3, axis=-1)
    x_val = np.repeat(x_val, 3, axis=-1)
    x_test = np.repeat(x_test, 3, axis=-1)

# ------------------------------------------------------------
# 3. Preprocessing Functions
# ------------------------------------------------------------
IMG_SIZE = 224
preprocess = keras.applications.efficientnet.preprocess_input

def make_ds(x, y, train=False, batch_size=64):
    ds = tf.data.Dataset.from_tensor_slices((x, y))

    if train:
        ds = ds.shuffle(10000, seed=SEED)

    # Resize + preprocess
    ds = ds.map(
        lambda a, b: (tf.image.resize(a, (IMG_SIZE, IMG_SIZE)), b),
        num_parallel_calls=tf.data.AUTOTUNE
    )
    ds = ds.map(
        lambda a, b: (preprocess(a), b),
        num_parallel_calls=tf.data.AUTOTUNE
    )

    # Light data augmentation
    if train:
        aug = keras.Sequential([
            layers.RandomFlip("horizontal"),
            layers.RandomRotation(0.05),
            layers.RandomZoom(0.1),
        ])
        ds = ds.map(
            lambda a, b: (aug(a, training=True), b),
            num_parallel_calls=tf.data.AUTOTUNE
        )

    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    return ds

train_ds = make_ds(x_train, y_train, train=True, batch_size=128)
val_ds   = make_ds(x_val,   y_val,   train=False, batch_size=128)
test_ds  = make_ds(x_test,  y_test,  train=False, batch_size=128)

# ------------------------------------------------------------
# 4. Build EfficientNetB0 Transfer Learning Model
# ------------------------------------------------------------
inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))

base = keras.applications.EfficientNetB0(
    include_top=False,
    weights=None,          # ‚Üê your script trains from scratch
    input_shape=(IMG_SIZE, IMG_SIZE, 3)
)

base.trainable = False

x = base(inputs, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dropout(0.3)(x)
outputs = layers.Dense(num_classes, activation="softmax")(x)

model_tl = keras.Model(inputs, outputs, name="efficientnetb0_cifar10")

model_tl.compile(
    optimizer=keras.optimizers.Adam(1e-3),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

model_tl.summary()

# ------------------------------------------------------------
# 5. WARMUP TRAINING (Train only the head)
# ------------------------------------------------------------
print("\n[Stage 1] Training head only...")

warmup = model_tl.fit(
    train_ds,
    validation_data=val_ds,
    epochs=5,
    verbose=2
)

# ------------------------------------------------------------
# 6. Fine-Tuning Last 50 Layers
# ------------------------------------------------------------
print("\n[Stage 2] Fine-tuning last 50 layers...")

for layer in base.layers[-50:]:
    if not isinstance(layer, layers.BatchNormalization):
        layer.trainable = True

model_tl.compile(
    optimizer=keras.optimizers.Adam(1e-4),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

early = keras.callbacks.EarlyStopping(
    patience=5,
    restore_best_weights=True,
    monitor="val_accuracy"
)

fine = model_tl.fit(
    train_ds,
    validation_data=val_ds,
    epochs=20,
    callbacks=[early],
    verbose=2
)

# ------------------------------------------------------------
# 7. Evaluation on Test Set
# ------------------------------------------------------------
print("\n[Stage 3] Evaluating on test set...")

probs = model_tl.predict(test_ds, verbose=0)
y_pred = probs.argmax(axis=1)

# Align true labels
y_true = np.concatenate([y.numpy() for _, y in test_ds], axis=0)

acc = (y_pred == y_true).mean()
print(f"[EfficientNetB0 TL] Test Accuracy: {acc:.4f}")

# Classification report
print("\nClassification Report:")
print(classification_report(y_true, y_pred, digits=4))

# Confusion Matrix
cm = confusion_matrix(y_true, y_pred)
print("\nConfusion Matrix:\n", cm)

# Macro & Micro F1
prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(
    y_true, y_pred, average='macro'
)
prec_micro, rec_micro, f1_micro, _ = precision_recall_fscore_support(
    y_true, y_pred, average='micro'
)

print(f"\nMacro Precision: {prec_macro:.4f} | Macro Recall: {rec_macro:.4f} | Macro F1: {f1_macro:.4f}")
print(f"Micro Precision: {prec_micro:.4f} | Micro Recall: {rec_micro:.4f} | Micro F1: {f1_micro:.4f}")

# ------------------------------------------------------------
# 8. ROC-AUC Score
# ------------------------------------------------------------
y_true_oh = keras.utils.to_categorical(y_true, num_classes)

try:
    auc_ovr = roc_auc_score(y_true_oh, probs, average="macro", multi_class="ovr")
    auc_ovo = roc_auc_score(y_true_oh, probs, average="macro", multi_class="ovo")
    print(f"\nROC-AUC (macro, OVR): {auc_ovr:.4f} | ROC-AUC (macro, OVO): {auc_ovo:.4f}")
except Exception as e:
    print("\nROC-AUC could not be computed:", e)
